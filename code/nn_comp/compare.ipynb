{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "from lib.cnnae_fully_convolutional import createLevel3FullyConvDropoutNet\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path_to_data = \"./data/reference/\"\n",
    "\n",
    "X_test = np.load(f\"{path_to_data}X.dat_test.npy\")\n",
    "Y_test = np.load(f\"{path_to_data}Y.dat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a channel dimension (for grayscale 1-channel data)\n",
    "# X_test = np.expand_dims(X_test, axis=1)\n",
    "# Y_test = np.expand_dims(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test[:, np.newaxis], dtype=torch.float32, device=device)\n",
    "Y_test_tensor = torch.tensor(Y_test[:, np.newaxis], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31455, 1, 7, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "batchsize = 64\n",
    "test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31455"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31424"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(test_loader.dataset) // batchsize) * batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 7, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, Y_test = next(iter(test_loader))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_test, Y_test = next(iter(test_loader))\n",
    "        if is_fully_connected:\n",
    "            # Reshape (batch_size, 1, 7, 7) to (batch_size, 49) for LargeNNv2\n",
    "            X_test = X_test.view(X_test.size(0), -1)\n",
    "        model(X_test)\n",
    "\n",
    "    # for X_test, Y_test in test_loader:\n",
    "         \n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    avg_time = total_time / len(test_loader.dataset)\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage(model, test_loader):\n",
    "    tracemalloc.start()\n",
    "\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_test, Y_test = next(iter(test_loader))\n",
    "        if is_fully_connected:\n",
    "            # Reshape (batch_size, 1, 7, 7) to (batch_size, 49) for LargeNNv2\n",
    "            X_test = X_test.view(X_test.size(0), -1)\n",
    "        model(X_test)\n",
    "\n",
    "    _, peak_memory = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    peak_memory_mb = peak_memory / (1024 * 1024)  # Convert to MB\n",
    "    return peak_memory_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_found_path(answer: torch.Tensor, Y_test: torch.Tensor):\n",
    "    start = find_gate(answer, 0.6, True)\n",
    "    end = find_gate(answer, 0.6, False)\n",
    "\n",
    "    if start is None or end is None:\n",
    "        return False\n",
    "\n",
    "    current = (start[0], start[1])\n",
    "    visited = set()\n",
    "    visited.add(current)\n",
    "\n",
    "    max_moves = (\n",
    "        answer.shape[0] * answer.shape[1]\n",
    "    )  # Consider the grid size for max moves\n",
    "    # max_moves = 17\n",
    "\n",
    "    for i in range(max_moves):\n",
    "        brightest_neighbour = find_brightest_neighbour(answer, current, visited, Y_test)\n",
    "        if brightest_neighbour is None:\n",
    "            return False\n",
    "        current = (brightest_neighbour[0], brightest_neighbour[1])\n",
    "        visited.add(current)\n",
    "        if current == end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_gate(answer: torch.Tensor, epsillon: float, start: bool):\n",
    "    rows, cols = answer.shape  # Get the actual number of rows and columns\n",
    "    for row in range(rows):\n",
    "        if start:\n",
    "            if answer[row][0] > (3 - epsillon):\n",
    "                return row, 0\n",
    "        else:\n",
    "            if answer[row][cols - 1] > (\n",
    "                3 - epsillon\n",
    "            ):  # Use the last column dynamically\n",
    "                return row, cols - 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_brightest_neighbour(\n",
    "    answer: torch.Tensor,\n",
    "    position: tuple[int, int],\n",
    "    visited: set[tuple[int, int]],\n",
    "    Y_test: torch.Tensor,\n",
    "):\n",
    "    rows, cols = answer.shape  # Get the actual number of rows and columns\n",
    "\n",
    "    ind_up = (max(position[0] - 1, 0), position[1])\n",
    "    ind_down = (min(position[0] + 1, rows - 1), position[1])\n",
    "    ind_left = (position[0], max(position[1] - 1, 0))\n",
    "    ind_right = (position[0], min(position[1] + 1, cols - 1))\n",
    "\n",
    "    value_up = answer[ind_up[0]][ind_up[1]] if ind_up not in visited else -1\n",
    "    value_down = answer[ind_down[0]][ind_down[1]] if ind_down not in visited else -1\n",
    "    value_left = answer[ind_left[0]][ind_left[1]] if ind_left not in visited else -1\n",
    "    value_right = answer[ind_right[0]][ind_right[1]] if ind_right not in visited else -1\n",
    "\n",
    "    # Find the maximum value among the neighbours and return the corresponding index\n",
    "    max_value = max(value_up, value_down, value_left, value_right)\n",
    "    if max_value == -1:\n",
    "        return None\n",
    "\n",
    "    if max_value == value_up and Y_test[ind_up[0]][ind_up[1]] != 1.0:\n",
    "        return ind_up\n",
    "    elif max_value == value_down and Y_test[ind_down[0]][ind_down[1]] != 1.0:\n",
    "        return ind_down\n",
    "    elif max_value == value_left and Y_test[ind_left[0]][ind_left[1]] != 1.0:\n",
    "        return ind_left\n",
    "    elif max_value == value_right and Y_test[ind_right[0]][ind_right[1]] != 1.0:\n",
    "        return ind_right\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_path_accuracy(model, test_loader: DataLoader):\n",
    "    preds_ = torch.zeros(len(test_loader.dataset), dtype=bool)\n",
    "\n",
    "    # Check the model type or input requirements\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_test, Y_test = next(iter(test_loader))\n",
    "        if is_fully_connected:\n",
    "            # Reshape (batch_size, 1, 7, 7) to (batch_size, 49) for LargeNNv2\n",
    "            X_test = X_test.view(X_test.size(0), -1)\n",
    "\n",
    "        # Pass the input through the model\n",
    "        Y_hat = model(X_test)\n",
    "\n",
    "        for index, (y_hat, y_true) in enumerate(zip(Y_hat, Y_test)):\n",
    "            # Reshape predictions and targets to 2D (7x7) for evaluation if needed\n",
    "            if is_fully_connected:\n",
    "                y_hat_reshaped = y_hat.view(7, 7)\n",
    "            else:\n",
    "                y_hat_reshaped = (\n",
    "                    y_hat.squeeze()\n",
    "                )  # For FCAE, remove channel dimension\n",
    "\n",
    "            y_true_reshaped = y_true.view(7, 7)\n",
    "\n",
    "            # Evaluate the path accuracy\n",
    "            preds_[index] = evaluate_found_path(y_hat_reshaped, y_true_reshaped)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = preds_.shape[0]\n",
    "    correct = preds_.sum().item()\n",
    "    correct_percentage = (correct / total) * 100\n",
    "\n",
    "    return correct_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(model1, model2, test_loader):\n",
    "    \"\"\"\n",
    "    Compare two models using metrics like path accuracy, path length error, inference time, and memory usage.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    # Evaluate Model 1\n",
    "    print(\"Evaluating Model 1:\")\n",
    "    model1.to(device)\n",
    "    model1.eval()\n",
    "    metrics[\"Model 1\"] = {\n",
    "        \"Accuracy\": evaluate_path_accuracy(model1, test_loader),\n",
    "        \"Inference Time (s)\": measure_inference_time(model1, test_loader),\n",
    "        \"Memory Usage (MB)\": measure_memory_usage(model1, test_loader),\n",
    "        # \"Path Length Errors\": evaluate_path_length_errors(model1, test_loader),\n",
    "    }\n",
    "\n",
    "    # Evaluate Model 2\n",
    "    print(\"\\nEvaluating Model 2:\")\n",
    "    model2.to(device)\n",
    "    model2.eval()\n",
    "    metrics[\"Model 2\"] = {\n",
    "        \"Accuracy\": evaluate_path_accuracy(model2, test_loader),\n",
    "        \"Inference Time (s)\": measure_inference_time(model2, test_loader),\n",
    "        \"Memory Usage (MB)\": measure_memory_usage(model2, test_loader),\n",
    "        # \"Path Length Errors\": evaluate_path_length_errors(model2, test_loader),\n",
    "    }\n",
    "\n",
    "    # Print Comparison\n",
    "    print(\"\\nComparison Summary:\")\n",
    "    for model_name, results in metrics.items():\n",
    "        print(f\"\\n{model_name} Results:\")\n",
    "        print(f\"  Accuracy: {results['Accuracy']:.2f}%\")\n",
    "        print(f\"  Inference Time: {results['Inference Time (s)']:.6f} seconds\")\n",
    "        print(f\"  Memory Usage: {results['Memory Usage (MB)']:.2f} MB\")\n",
    "        # print(\n",
    "        #     f\"  Avg Path Length Absolute Error: {results['Path Length Errors'][0]:.2f}\"\n",
    "        # )\n",
    "        # print(\n",
    "        #     f\"  Avg Path Length Percentage Error: {results['Path Length Errors'][1]:.2f}%\"\n",
    "        # )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create FCNN\n",
    "class LargeNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNNv2, self).__init__()\n",
    "        self.fc1 = nn.Linear(49, 1235)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1235, 768)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(768, 532)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(532, 149)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(149, 98)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(98, 49)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.relu5(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "fcnn = LargeNNv2()\n",
    "\n",
    "fcnn.load_state_dict(torch.load(\"./reference/model/collaboratory/m_300.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models\n",
    "fcae, _, _ = createLevel3FullyConvDropoutNet((15, 84, 167))\n",
    "\n",
    "fcae.load_state_dict(\n",
    "    torch.load(\n",
    "        \"./archive/Level3FullyConvDropoutNet_99.8647/net.pt\",\n",
    "        weights_only=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [3., 0., 0.,  ..., 0., 0., 3.],\n",
       "          ...,\n",
       "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 1., 0.],\n",
       "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "          ...,\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [3., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 3.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "          ...,\n",
       "          [3., 0., 0.,  ..., 0., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 1.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[3., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 0., 0.,  ..., 0., 0., 3.],\n",
       "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 0.,  ..., 1., 0., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 0., 1., 1.],\n",
       "          [0., 0., 0.,  ..., 1., 0., 3.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 0., 0., 1.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 1., 1.,  ..., 0., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 0., 0.,  ..., 1., 0., 3.],\n",
       "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
       "          ...,\n",
       "          [3., 0., 0.,  ..., 1., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 1., 1., 0.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcae.to(\"cuda\")\n",
    "X_test_tensor.to(\"cuda\")\n",
    "# output = fcae(X_test_tensor)\n",
    "# correct_percentage = evaluate_path_accuracy(fcae, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "outputs_truth = []\n",
    "\n",
    "for X_batch, Y_batch in dataloader:\n",
    "    X_batch.to(\"cuda\")\n",
    "    # Y_batch.to(\"cuda\")\n",
    "    output = fcae(X_batch)\n",
    "    outputs.append(output)\n",
    "    outputs_truth.append(Y_batch)\n",
    "\n",
    "result = torch.cat(outputs, dim=0)\n",
    "result_truth = torch.cat(outputs_truth, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31424, 1, 7, 7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 7, 7])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------MODEL SUMMARY-----------\n",
      "TOTAL SET SIZE:  31424\n",
      "CORRECT GUESSES:  31031\n",
      "TOTALING TO ACCURACY%:  98.74936354378818\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lib.evaluate import evaluate_total_predictions_set\n",
    "\n",
    "correct_percentage = evaluate_total_predictions_set(result.cpu(), result_truth.cpu(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor_reshaped = X_test_tensor.view(X_test_tensor.size(0), 1, -1)\n",
    "Y_test_tensor_reshaped = Y_test_tensor.view(Y_test_tensor.size(0), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31455, 1, 49])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tensor_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reshaped = TensorDataset(X_test_tensor_reshaped, Y_test_tensor_reshaped)\n",
    "dataloader_reshaped = DataLoader(dataset_reshaped, batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2 = []\n",
    "outputs2_truth = []\n",
    "fcnn.to(\"cuda\")\n",
    "\n",
    "for X_batch, Y_batch in dataloader_reshaped:\n",
    "    X_batch.to(\"cuda\")\n",
    "    # Y_batch.to(\"cuda\")\n",
    "    output = fcnn(X_batch)\n",
    "    outputs2.append(output)\n",
    "    outputs2_truth.append(Y_batch)\n",
    "\n",
    "result = torch.cat(outputs2, dim=0)\n",
    "result_truth = torch.cat(outputs2_truth, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcnn.to(\"cuda\")\n",
    "# X_test_tensor_reshaped.to(\"cuda\")\n",
    "# output2 = fcnn(X_test_tensor_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31424, 1, 49])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31424, 1, 7, 7])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_reshaped = result.view(result.size(0), 1, 7, 7)\n",
    "y_hat_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31424, 1, 7, 7])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truth_reshaped = result_truth.view(result_truth.size(0), 1, 7, 7)\n",
    "y_truth_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------MODEL SUMMARY-----------\n",
      "TOTAL SET SIZE:  31424\n",
      "CORRECT GUESSES:  31377\n",
      "TOTALING TO ACCURACY%:  99.85043279022403\n",
      "------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_percentage2 = evaluate_total_predictions_set(\n",
    "    y_hat_reshaped.cpu(), y_truth_reshaped.cpu(), verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model 1:\n",
      "\n",
      "Evaluating Model 2:\n",
      "\n",
      "Comparison Summary:\n",
      "\n",
      "Model 1 Results:\n",
      "  Accuracy: 99.81%\n",
      "  Inference Time: 0.000018 seconds\n",
      "  Memory Usage: 10.60 MB\n",
      "\n",
      "Model 2 Results:\n",
      "  Accuracy: 99.85%\n",
      "  Inference Time: 0.000012 seconds\n",
      "  Memory Usage: 10.61 MB\n"
     ]
    }
   ],
   "source": [
    "# metrics = compare_models(fcae, fcnn, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
