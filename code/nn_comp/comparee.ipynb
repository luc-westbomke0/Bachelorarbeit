{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tabulate import tabulate\n",
    "from torchinfo import summary\n",
    "\n",
    "from lib.cnnae_fully_convolutional import createLevel3FullyConvDropoutNet\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path_to_data = \"./data/reference/\"\n",
    "\n",
    "X_test = np.load(f\"{path_to_data}X.dat_test.npy\")\n",
    "Y_test = np.load(f\"{path_to_data}Y.dat_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test[:, np.newaxis], dtype=torch.float32, device=device)\n",
    "Y_test_tensor = torch.tensor(Y_test[:, np.newaxis], dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "batchsize = 1024\n",
    "# batchsize = test_data.tensors[0].shape[0]\n",
    "test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FCNN\n",
    "class LargeNNv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LargeNNv2, self).__init__()\n",
    "        self.fc1 = nn.Linear(49, 1235)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1235, 768)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(768, 532)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(532, 149)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(149, 98)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(98, 49)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.relu4(self.fc4(x))\n",
    "        x = self.relu5(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models\n",
    "channel_configuration = (15, 24, 33)\n",
    "fcae, _, _ = createLevel3FullyConvDropoutNet(channel_configuration)\n",
    "\n",
    "fcae.load_state_dict(\n",
    "    torch.load(\n",
    "        \"./archive/Level3FullyConvDropoutNet_99.6498/net.pt\",\n",
    "        weights_only=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "fcnn = LargeNNv2()\n",
    "\n",
    "fcnn.load_state_dict(\n",
    "    torch.load(\"./reference/model/collaboratory/m_300.pth\", weights_only=True)\n",
    ")\n",
    "\n",
    "# fcnn2 = LargeNNv2()\n",
    "\n",
    "# fcnn2.load_state_dict(\n",
    "#     torch.load(\"./reference/model/collaboratory/ms-13-collab.pth\", weights_only=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_event_measure_per_sample(model, test_loader):\n",
    "    \"\"\"\n",
    "    Misst die durchschnittliche Inferenzzeit pro Datenpunkt eines Modells auf der GPU.\n",
    "\n",
    "    Args:\n",
    "        model: Das zu testende Modell (z.B. LargeNNv2 oder Level3FullyConvDropoutNet).\n",
    "        test_loader: Dataloader mit Testdaten.\n",
    "\n",
    "    Returns:\n",
    "        avg_time_per_sample: Durchschnittliche Inferenzzeit pro Datenpunkt in Mikrosekunden.\n",
    "        runtimes_per_sample: Liste der gemessenen Zeiten pro Datenpunkt.\n",
    "    \"\"\"\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    runtimes_per_sample = []\n",
    "\n",
    "    # Modell vorbereiten\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):  # Warm-up Runs\n",
    "            for x_test, _ in test_loader:\n",
    "                if is_fully_connected:\n",
    "                    x_test = x_test.view(x_test.size(0), -1).to(device)\n",
    "                else:\n",
    "                    x_test = x_test.to(device)\n",
    "                model(x_test)\n",
    "\n",
    "        for _ in range(10):  # Messungen\n",
    "            for x_test, _ in test_loader:\n",
    "                batch_size = x_test.size(0)\n",
    "                if is_fully_connected:\n",
    "                    x_test = x_test.view(batch_size, -1).to(device)\n",
    "                else:\n",
    "                    x_test = x_test.to(device)\n",
    "\n",
    "                start.record()\n",
    "                model(x_test)\n",
    "                end.record()\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time_per_batch = start.elapsed_time(end)  # Zeit in ms\n",
    "                elapsed_time_per_sample = (\n",
    "                    elapsed_time_per_batch / batch_size\n",
    "                )  # Zeit pro Datenpunkt\n",
    "                runtimes_per_sample.append(elapsed_time_per_sample * 1000) # milliseconds to microseconds\n",
    "\n",
    "    avg_time_per_sample = sum(runtimes_per_sample) / len(runtimes_per_sample)\n",
    "    return avg_time_per_sample, runtimes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader):\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "    runtimes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, Y in test_loader:\n",
    "            if is_fully_connected:\n",
    "                # Reshape (batch_size, 1, 7, 7) to (batch_size, 49) for LargeNNv2\n",
    "                x_test = x_test.view(x_test.size(0), 1, -1)\n",
    "\n",
    "            start_time = time.perf_counter_ns()\n",
    "            model(x_test)\n",
    "            total_time = time.perf_counter_ns() - start_time\n",
    "            runtimes.append(total_time)\n",
    "\n",
    "    avg_time = sum(runtimes) / len(test_loader.dataset)\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "    \n",
    "    peak_memory = torch.cuda.max_memory_allocated()  # GPU Peak Memory\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, _ in test_loader:\n",
    "            if is_fully_connected:\n",
    "                x_test = x_test.view(x_test.size(0), -1).to(device)\n",
    "            else:\n",
    "                x_test = x_test.to(device)\n",
    "            model(x_test)\n",
    "\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / (1024**2)  # In MB\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    return peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_found_path(answer: torch.Tensor, Y_test: torch.Tensor):\n",
    "    start = find_gate(answer, 0.6, True)\n",
    "    end = find_gate(answer, 0.6, False)\n",
    "\n",
    "    if start is None or end is None:\n",
    "        return False\n",
    "\n",
    "    current = (start[0], start[1])\n",
    "    visited = set()\n",
    "    visited.add(current)\n",
    "\n",
    "    max_moves = (\n",
    "        answer.shape[0] * answer.shape[1]\n",
    "    )  # Consider the grid size for max moves\n",
    "    # max_moves = 17\n",
    "\n",
    "    for i in range(max_moves):\n",
    "        brightest_neighbour = find_brightest_neighbour(answer, current, visited, Y_test)\n",
    "        if brightest_neighbour is None:\n",
    "            return False\n",
    "        current = (brightest_neighbour[0], brightest_neighbour[1])\n",
    "        visited.add(current)\n",
    "        if current == end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_gate(answer: torch.Tensor, epsillon: float, start: bool):\n",
    "    rows, cols = answer.shape  # Get the actual number of rows and columns\n",
    "    for row in range(rows):\n",
    "        if start:\n",
    "            if answer[row][0] > (3 - epsillon):\n",
    "                return row, 0\n",
    "        else:\n",
    "            if answer[row][cols - 1] > (\n",
    "                3 - epsillon\n",
    "            ):  # Use the last column dynamically\n",
    "                return row, cols - 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_brightest_neighbour(\n",
    "    answer: torch.Tensor,\n",
    "    position: tuple[int, int],\n",
    "    visited: set[tuple[int, int]],\n",
    "    Y_test: torch.Tensor,\n",
    "):\n",
    "    rows, cols = answer.shape  # Get the actual number of rows and columns\n",
    "\n",
    "    ind_up = (max(position[0] - 1, 0), position[1])\n",
    "    ind_down = (min(position[0] + 1, rows - 1), position[1])\n",
    "    ind_left = (position[0], max(position[1] - 1, 0))\n",
    "    ind_right = (position[0], min(position[1] + 1, cols - 1))\n",
    "\n",
    "    value_up = answer[ind_up[0]][ind_up[1]] if ind_up not in visited else -1\n",
    "    value_down = answer[ind_down[0]][ind_down[1]] if ind_down not in visited else -1\n",
    "    value_left = answer[ind_left[0]][ind_left[1]] if ind_left not in visited else -1\n",
    "    value_right = answer[ind_right[0]][ind_right[1]] if ind_right not in visited else -1\n",
    "\n",
    "    # Find the maximum value among the neighbours and return the corresponding index\n",
    "    max_value = max(value_up, value_down, value_left, value_right)\n",
    "    if max_value == -1:\n",
    "        return None\n",
    "\n",
    "    if max_value == value_up and Y_test[ind_up[0]][ind_up[1]] != 1.0:\n",
    "        return ind_up\n",
    "    elif max_value == value_down and Y_test[ind_down[0]][ind_down[1]] != 1.0:\n",
    "        return ind_down\n",
    "    elif max_value == value_left and Y_test[ind_left[0]][ind_left[1]] != 1.0:\n",
    "        return ind_left\n",
    "    elif max_value == value_right and Y_test[ind_right[0]][ind_right[1]] != 1.0:\n",
    "        return ind_right\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_path_accuracy(model, test_loader: DataLoader):\n",
    "    preds_ = torch.zeros(len(test_loader.dataset), dtype=bool)\n",
    "\n",
    "    # Check the model type or input requirements\n",
    "    is_fully_connected = isinstance(model, LargeNNv2)\n",
    "    x = next(iter(test_loader))[0]\n",
    "    rows, cols = (\n",
    "        x.shape[2],\n",
    "        x.shape[3],\n",
    "    )  # Assumes Y_hat has shape (batch_size, channels, rows, cols)\n",
    "\n",
    "    output = []\n",
    "    output_truth = []\n",
    "    # i = 0\n",
    "    with torch.no_grad():\n",
    "        for x_test, Y in test_loader:\n",
    "            if is_fully_connected:\n",
    "                # Reshape (batch_size, 1, 7, 7) to (batch_size, 1, 49) for LargeNNv2\n",
    "                x_test = x_test.view(x_test.size(0), 1, -1)\n",
    "\n",
    "            # Pass the input through the model\n",
    "            y_hat = model(x_test)\n",
    "            output.append(y_hat)\n",
    "            output_truth.append(Y)\n",
    "            # i += 1\n",
    "            # print(f\"got {i} batches from model\")\n",
    "\n",
    "    output = torch.cat(output, dim=0).cpu()\n",
    "    output_truth = torch.cat(output_truth, dim=0).cpu()\n",
    "\n",
    "    for index, (y_hat, y_true) in enumerate(zip(output, output_truth)):\n",
    "        # Reshape predictions and targets to 2D (7x7) for evaluation if needed\n",
    "        if is_fully_connected:\n",
    "            y_hat = y_hat.view(y_hat.size(0), 1, 7, 7)\n",
    "\n",
    "        # Evaluate the path accuracy\n",
    "        preds_[index] = evaluate_found_path(\n",
    "            y_hat.view(rows, cols), y_true.view(rows, cols)\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = preds_.shape[0]\n",
    "    correct = preds_.sum().item()\n",
    "    correct_percentage = (correct / total) * 100\n",
    "\n",
    "    return correct_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_summary(model):\n",
    "    \"\"\"\n",
    "    Liefert Informationen über die Anzahl der Parameter und die Modellgröße.\n",
    "    \"\"\"\n",
    "    param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size = (\n",
    "        param_count * 4 / (1024**2)\n",
    "    )  # Float32 ist 4 Bytes groß, MB-Konvertierung\n",
    "    summary(model)\n",
    "    return param_count, model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_extended(test_loader, *models):\n",
    "    \"\"\"\n",
    "    Vergleicht Modelle basierend auf erweiterten Metriken.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        print(f\"Evaluating Model {model.__class__.__name__} {idx}\")\n",
    "        model_name = f\"{model.__class__.__name__} {idx}\"\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        # Path Accuracy\n",
    "        accuracy = evaluate_path_accuracy(model, test_loader)\n",
    "        # Inference Time per Sample\n",
    "        avg_time_per_sample, _ = cuda_event_measure_per_sample(model, test_loader)\n",
    "        # Memory Usage\n",
    "        memory_usage = measure_memory_usage(model, test_loader)\n",
    "        # Parameter Count & Model Size\n",
    "        param_count, model_size = get_model_summary(model)\n",
    "\n",
    "        # Ergebnisse sammeln\n",
    "        results.append(\n",
    "            {\n",
    "                \"Model\": model_name,\n",
    "                \"Path Accuracy (%)\": accuracy,\n",
    "                \"Inference Time (µs)\": avg_time_per_sample,\n",
    "                \"Memory Usage (MB)\": memory_usage,\n",
    "                \"Parameter Count\": param_count,\n",
    "                \"Model Size (MB)\": model_size,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Ergebnisse als DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_table(metrics_df):\n",
    "    \"\"\"\n",
    "    Generiert eine Tabelle für den Vergleich der Modelle mit zusätzlichen Metriken.\n",
    "    \"\"\"\n",
    "    # Tabelle erstellen\n",
    "    table = tabulate(metrics_df, headers=\"keys\", tablefmt=\"grid\", showindex=False)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(summary(fcnn.to(device), (64, 1, 1, 49)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Model Level3FullyConvDropoutNet 0\n",
      "Evaluating Model LargeNNv2 1\n",
      "                         Model  Path Accuracy (%)  Inference Time (µs)  \\\n",
      "0  Level3FullyConvDropoutNet 0          97.211890             2.371730   \n",
      "1                  LargeNNv2 1          97.513909             1.989789   \n",
      "\n",
      "   Memory Usage (MB)  Parameter Count  Model Size (MB)  \n",
      "0          47.915039            21340         0.081406  \n",
      "1          37.049805          1519074         5.794807  \n"
     ]
    }
   ],
   "source": [
    "# Beispiel:\n",
    "metrics_df = compare_models_extended(test_loader, fcae, fcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiere und drucke die Tabelle\n",
    "comparison_table = generate_comparison_table(metrics_df)\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "# test_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# batchsize = 64\n",
    "# batchsize = test_data.tensors[0].shape[0]\n",
    "# test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "x_test = X_test_tensor.detach().clone()\n",
    "x_test = x_test.to(device)\n",
    "\n",
    "# model = fcnn.eval().to(device)\n",
    "model = fcae.eval().to(device)\n",
    "if isinstance(model, LargeNNv2):\n",
    "    x_test = x_test.view(x_test.size(0), 1, -1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.profiler.profile(\n",
    "        activities=[\n",
    "            torch.profiler.ProfilerActivity.CPU,\n",
    "            torch.profiler.ProfilerActivity.CUDA,\n",
    "        ],\n",
    "        # schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),\n",
    "        # record_shapes=True,\n",
    "        # with_stack=True,\n",
    "        profile_memory=True,\n",
    "        with_flops=True,\n",
    "        # with_modules=True,\n",
    "    ) as prof:\n",
    "        model(x_test)\n",
    "        # with torch.profiler.record_function(\"model_inference\"):\n",
    "\n",
    "print(\n",
    "    prof.key_averages(group_by_input_shape=True).table(\n",
    "        sort_by=\"cuda_time_total\",\n",
    "        row_limit=-1,  # Erhöhe die Anzahl der angezeigten Zeilen\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
