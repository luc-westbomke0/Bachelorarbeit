{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tracemalloc\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "\n",
    "from model.fcae import createLevel6Net_10x10, createLevel7Net_20x20, createLevel8Net_30x30, createLevel8Net_40x40, createLevel8Net_50x50\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(maze):\n",
    "    rows, cols = maze.shape\n",
    "    # Find the start and end gates (positions marked as '3')\n",
    "    gates = np.argwhere(maze == (3.0 - 0.0) / (3.0 - 0.0))\n",
    "    if len(gates) < 2:\n",
    "        return maze  # If we don't have at least two gates, there's nothing to solve\n",
    "\n",
    "    start = tuple(gates[0])\n",
    "    end = tuple(gates[1])\n",
    "\n",
    "    # Helper function to compute heuristic (Manhattan distance)\n",
    "    def heuristic(a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    # Directions for movement (up, down, left, right)\n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "    # Open set and cost tracking\n",
    "    open_set = [start]\n",
    "    came_from = {}\n",
    "    g_score = {start: 0}\n",
    "    f_score = {start: heuristic(start, end)}\n",
    "\n",
    "    while open_set:\n",
    "        # Find the node in open set with the lowest f_score\n",
    "        current = min(open_set, key=lambda x: f_score.get(x, float(\"inf\")))\n",
    "        open_set.remove(current)\n",
    "\n",
    "        if current == end:\n",
    "            # Reconstruct path and remove obstacles (walls)\n",
    "            solved_maze = np.where(maze == (1.0 - 0.0) / (3.0 - 0.0), 0, maze)  # Remove walls (obstacles)\n",
    "            x, y = current\n",
    "            while current in came_from:\n",
    "                x, y = current\n",
    "                if solved_maze[x, y] == 0:\n",
    "                    solved_maze[x, y] = 2  # Mark the path with '2'\n",
    "                current = came_from[current]\n",
    "            return solved_maze\n",
    "\n",
    "        # Explore neighbors\n",
    "        for dx, dy in directions:\n",
    "            neighbor = (current[0] + dx, current[1] + dy)\n",
    "\n",
    "            if (\n",
    "                0 <= neighbor[0] < rows\n",
    "                and 0 <= neighbor[1] < cols\n",
    "                and maze[neighbor] != ((1.0 - 0.0) / (3.0 - 0.0))\n",
    "            ):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "\n",
    "                if tentative_g_score < g_score.get(neighbor, float(\"inf\")):\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = tentative_g_score + heuristic(neighbor, end)\n",
    "                    if neighbor not in open_set:\n",
    "                        open_set.append(neighbor)\n",
    "\n",
    "    return maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_found_path(answer: torch.Tensor, Y_test: torch.Tensor):\n",
    "    start = find_gate(Y_test, 0.8, True, 3)\n",
    "    end = find_gate(Y_test, 0.8, False, 3)\n",
    "\n",
    "    if start is None or end is None:\n",
    "        return False\n",
    "\n",
    "    current = (start[0], start[1])\n",
    "    visited = set()\n",
    "    visited.add(current)\n",
    "\n",
    "    max_moves = (\n",
    "        answer.shape[0] * answer.shape[1]\n",
    "    )  # Consider the grid size for max moves\n",
    "    # max_moves = 17\n",
    "\n",
    "    for i in range(max_moves):\n",
    "        brightest_neighbour = find_brightest_neighbour(answer, current, visited, Y_test)\n",
    "        if brightest_neighbour is None:\n",
    "            return False\n",
    "        current = (brightest_neighbour[0], brightest_neighbour[1])\n",
    "        visited.add(current)\n",
    "        if current == end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_gate(answer: torch.Tensor, threshold: float, start: bool, gate_value: float = 1.0):\n",
    "    # Definiere den Bereich, in dem nach dem Gate gesucht wird\n",
    "    min_value = gate_value - threshold\n",
    "    max_value = gate_value + threshold\n",
    "\n",
    "    # Finde alle Kandidaten, die im Bereich [min_value, max_value] liegen\n",
    "    gate_candidates = ((answer >= min_value) & (answer <= max_value)).nonzero(as_tuple=True)\n",
    "\n",
    "    # Prüfe, ob Kandidaten existieren\n",
    "    if len(gate_candidates[0]) == 0:\n",
    "        return None  # Kein Gate gefunden\n",
    "\n",
    "    # Wähle das erste oder letzte Gate aus\n",
    "    if start:\n",
    "        return gate_candidates[0][0].item(), gate_candidates[1][0].item()  # Erstes Gate\n",
    "    else:\n",
    "        return gate_candidates[0][-1].item(), gate_candidates[1][-1].item()  # Letztes Gate\n",
    "\n",
    "def find_brightest_neighbour(\n",
    "    answer: torch.Tensor,\n",
    "    position: tuple[int, int],\n",
    "    visited: set[tuple[int, int]],\n",
    "    Y_test: torch.Tensor,\n",
    "):\n",
    "    rows, cols = answer.shape  # Get the actual number of rows and columns\n",
    "\n",
    "    ind_up = (max(position[0] - 1, 0), position[1])\n",
    "    ind_down = (min(position[0] + 1, rows - 1), position[1])\n",
    "    ind_left = (position[0], max(position[1] - 1, 0))\n",
    "    ind_right = (position[0], min(position[1] + 1, cols - 1))\n",
    "\n",
    "    value_up = answer[ind_up[0]][ind_up[1]] if ind_up not in visited else -1\n",
    "    value_down = answer[ind_down[0]][ind_down[1]] if ind_down not in visited else -1\n",
    "    value_left = answer[ind_left[0]][ind_left[1]] if ind_left not in visited else -1\n",
    "    value_right = answer[ind_right[0]][ind_right[1]] if ind_right not in visited else -1\n",
    "\n",
    "    # Find the maximum value among the neighbours and return the corresponding index\n",
    "    max_value = max(value_up, value_down, value_left, value_right)\n",
    "    if max_value == -1:\n",
    "        return None\n",
    "\n",
    "    if max_value == value_up and Y_test[ind_up[0]][ind_up[1]] != 1.0:\n",
    "        return ind_up\n",
    "    elif max_value == value_down and Y_test[ind_down[0]][ind_down[1]] != 1.0:\n",
    "        return ind_down\n",
    "    elif max_value == value_left and Y_test[ind_left[0]][ind_left[1]] != 1.0:\n",
    "        return ind_left\n",
    "    elif max_value == value_right and Y_test[ind_right[0]][ind_right[1]] != 1.0:\n",
    "        return ind_right\n",
    "\n",
    "    return None\n",
    "\n",
    "def measure_accuracy(model, test_loader: DataLoader, algorithm_fn=None):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of a model or an algorithm (e.g., A*) in solving mazes.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate (can be None if only algorithm_fn is used).\n",
    "        test_loader: DataLoader containing the test data (input mazes and their ground truth solutions).\n",
    "        algorithm_fn: A function implementing the A* algorithm (if provided, it will be used instead of the model).\n",
    "\n",
    "    Returns:\n",
    "        correct_percentage: Accuracy as a percentage of correctly solved mazes.\n",
    "    \"\"\"\n",
    "    preds_ = torch.zeros(len(test_loader.dataset), dtype=bool)  # To store correctness of each maze\n",
    "    rows, cols = test_loader.dataset[0][0].shape[-2:]  # Determine maze dimensions\n",
    "\n",
    "    output = []\n",
    "    output_truth = []\n",
    "\n",
    "    # Collect predictions from the model or the A* algorithm\n",
    "    with torch.no_grad():\n",
    "        if model is not None:\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            for x_test, y_true in test_loader:\n",
    "                # Pass the input through the model\n",
    "                x_test = x_test.to(device)\n",
    "                y_hat = model(x_test)\n",
    "                output.append(y_hat)\n",
    "                output_truth.append(y_true)\n",
    "        elif algorithm_fn is not None:\n",
    "            # Pass the input through the A* algorithm\n",
    "            y_hat_batch = []\n",
    "            for x_test, y_true in test_loader:\n",
    "                for maze in x_test:\n",
    "                    maze = maze.squeeze().numpy()\n",
    "                    y_hat_batch.append(torch.tensor(algorithm_fn(maze), dtype=torch.float32))\n",
    "                output.append(torch.stack(y_hat_batch))\n",
    "                output_truth.append(y_true)\n",
    "\n",
    "    # Concatenate predictions and ground truths\n",
    "    output = torch.cat(output, dim=0).cpu()\n",
    "    output_truth = torch.cat(output_truth, dim=0).cpu()\n",
    "\n",
    "    print(\"Evalutating accuracy\")\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    for index, (y_hat, y_true) in enumerate(zip(output, output_truth)):\n",
    "        if index % 200 == 0:\n",
    "            print(f\"testing maze {index}\")\n",
    "        # Evaluate the path accuracy\n",
    "        preds_[index] = evaluate_found_path(\n",
    "            y_hat.view(rows, cols), y_true.view(rows, cols)\n",
    "        )\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = preds_.shape[0]\n",
    "    correct = preds_.sum().item()\n",
    "    correct_percentage = (correct / total) * 100\n",
    "\n",
    "    return correct_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time_nn(model, test_loader):\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    runtimes_per_sample = []\n",
    "\n",
    "    # Modell vorbereiten\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(2):  # Warm-up Runs\n",
    "            for x_test, _ in test_loader:\n",
    "                x_test = x_test.to(device)\n",
    "                model(x_test)\n",
    "\n",
    "        for _ in range(5):  # \n",
    "            print(f\"round {_}\")\n",
    "            for x_test, _ in test_loader:\n",
    "                batch_size = x_test.size(0)\n",
    "                x_test = x_test.to(device)\n",
    "\n",
    "                start.record()\n",
    "                model(x_test)\n",
    "                end.record()\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time_per_batch = start.elapsed_time(end)\n",
    "                elapsed_time_per_sample = (\n",
    "                    elapsed_time_per_batch / batch_size\n",
    "                )\n",
    "                runtimes_per_sample.append(elapsed_time_per_sample * 1000) # milliseconds to microseconds\n",
    "\n",
    "    avg_time_per_sample = sum(runtimes_per_sample) / len(runtimes_per_sample)\n",
    "    return avg_time_per_sample, runtimes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time_nn_cpu(model, test_loader):\n",
    "    runtimes_per_sample = []\n",
    "\n",
    "    # Modell vorbereiten\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):  # \n",
    "            print(f\"round {_}\")\n",
    "            for x_test, _ in test_loader:\n",
    "                batch_size = x_test.size(0)\n",
    "\n",
    "                start_time = time.perf_counter_ns()\n",
    "                model(x_test)\n",
    "                elapsed_time_per_batch = time.perf_counter_ns() - start_time\n",
    "                \n",
    "                elapsed_time_per_sample = (\n",
    "                    elapsed_time_per_batch / batch_size\n",
    "                )\n",
    "                runtimes_per_sample.append(elapsed_time_per_sample / 1000)\n",
    "\n",
    "    avg_time_per_sample = sum(runtimes_per_sample) / len(runtimes_per_sample)\n",
    "    return avg_time_per_sample, runtimes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time_alg(algorithm_fn, test_loader):\n",
    "    \"\"\"\n",
    "    Measures the average inference time per maze for the A* algorithm on the CPU.\n",
    "\n",
    "    Args:\n",
    "        algorithm_fn: The A* function that takes a single maze as input.\n",
    "        test_loader: Dataloader containing batches of mazes.\n",
    "\n",
    "    Returns:\n",
    "        avg_time_per_sample: Average inference time per maze in microseconds.\n",
    "        runtimes_per_sample: List of measured times per maze in microseconds.\n",
    "    \"\"\"\n",
    "    runtimes_per_sample = []\n",
    "\n",
    "    # Warm-up Runs (for system consistency)\n",
    "    # for _ in range(5):\n",
    "    #     for x_batch, _ in test_loader:\n",
    "    #         for maze in x_batch:\n",
    "    #             algorithm_fn(maze.squeeze().cpu().numpy())  # Ensure A* runs on the CPU\n",
    "\n",
    "    # Actual Measurements\n",
    "    with torch.no_grad():\n",
    "        for _ in range(5):  # Number of measurement runs\n",
    "            print(f\"round {_}\")\n",
    "            for x_batch, _ in test_loader:\n",
    "                batch_times = []\n",
    "                x_batch = x_batch.squeeze().numpy()\n",
    "                for maze in x_batch:\n",
    "                    start_time = time.perf_counter_ns()  # Start timing in nanoseconds\n",
    "                    algorithm_fn(maze)\n",
    "                    elapsed_time = time.perf_counter_ns() - start_time  # Time in nanoseconds\n",
    "                    batch_times.append(elapsed_time / 1_000)  # Convert to microseconds\n",
    "                runtimes_per_sample.extend(batch_times)\n",
    "\n",
    "    avg_time_per_sample = sum(runtimes_per_sample) / len(runtimes_per_sample)\n",
    "    return avg_time_per_sample, runtimes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage_nn(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device)  # GPU Peak Memory\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, _ in test_loader:\n",
    "            x_test = x_test.to(device)\n",
    "            model(x_test)\n",
    "\n",
    "    peak_memory = torch.cuda.max_memory_allocated(device) / (1024**2)  # In MB\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    return peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def measure_memory_usage_nn_cpu(model, test_loader):\n",
    "#     model.eval()\n",
    "\n",
    "#     tracemalloc.start()\n",
    "#     tracemalloc.reset_peak()  # Reset memory stats before function call\n",
    "#     model.to(\"cpu\")\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for x_test, _ in test_loader:\n",
    "    #         # x_test = x_test.to(device)\n",
    "    #         model(x_test)\n",
    "\n",
    "#     # Get peak memory usage\n",
    "#     _, peak_memory = tracemalloc.get_traced_memory()\n",
    "#     tracemalloc.stop()\n",
    "\n",
    "#     # Convert to MB and return the memory usage in MB\n",
    "#     return peak_memory / (1024**2)\n",
    "\n",
    "def measure_memory_usage_nn_cpu(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to(\"cpu\")\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_memory = process.memory_info().rss  # In Bytes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_test, _ in test_loader:\n",
    "            model(x_test)  # Simulate forward pass\n",
    "\n",
    "    end_memory = process.memory_info().rss  # In Bytes\n",
    "    peak_memory = (end_memory - start_memory) / (1024**2)  # Convert to MB\n",
    "    \n",
    "    return peak_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_memory_usage_alg(algorithm_fn, test_loader):\n",
    "    tracemalloc.start()\n",
    "    tracemalloc.reset_peak()  # Reset memory stats before function call\n",
    "\n",
    "    # Call the function\n",
    "    i = 0\n",
    "    for x_batch, _ in test_loader:\n",
    "        x_batch = x_batch.squeeze().numpy()\n",
    "        for maze in x_batch:\n",
    "            if i % 200 == 0:\n",
    "                print(f\"memory usage for {i}\")\n",
    "\n",
    "            i += 1\n",
    "            algorithm_fn(maze)\n",
    "\n",
    "    # Get peak memory usage\n",
    "    _, peak_memory = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Convert to MB and return the memory usage in MB\n",
    "    return peak_memory / (1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_nn_alg(test_loader, model, algorithm_fn):\n",
    "    \"\"\"\n",
    "    Vergleicht Modelle basierend auf erweiterten Metriken.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Path Accuracy\n",
    "    print(f\"nn_accuracy\")\n",
    "    nn_accuracy = measure_accuracy(model, test_loader)\n",
    "    print(f\"alg_accuracy\")\n",
    "    # alg_accuracy = measure_accuracy(None, test_loader, algorithm_fn)\n",
    "\n",
    "    # Inference Time per Sample\n",
    "    print(f\"nn_avg_inference_time_gpu\")\n",
    "    nn_avg_inference_time_gpu, _ = measure_inference_time_nn(model, test_loader)\n",
    "    print(f\"nn_avg_inference_time_cpu\")\n",
    "    nn_avg_inference_time_cpu, _ = measure_inference_time_nn_cpu(model, test_loader)\n",
    "    print(f\"alg_avg_inference_time\")\n",
    "    alg_avg_inference_time, _ = measure_inference_time_alg(algorithm_fn, test_loader)\n",
    "\n",
    "    # Memory Usage\n",
    "    print(f\"nn_memory_usage_gpu\")\n",
    "    nn_memory_usage_gpu = measure_memory_usage_nn(model, test_loader)\n",
    "    print(f\"nn_memory_usage_cpu\")\n",
    "    nn_memory_usage_cpu = measure_memory_usage_nn_cpu(model, test_loader)\n",
    "    print(f\"alg_memory_usage\")\n",
    "    alg_memory_usage = measure_memory_usage_alg(algorithm_fn, test_loader)\n",
    "    # Parameter Count & Model Size\n",
    "    # param_count, model_size = get_model_summary(model)\n",
    "\n",
    "    data_size = test_loader.dataset[0][0].shape[-2:]\n",
    "\n",
    "    # # Ergebnisse sammeln\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": f\"FCAE {model.__class__.__name__}\",\n",
    "            \"Maze size (px)\": f\"{data_size[0]}x{data_size[1]}\",\n",
    "            \"Path Accuracy (%)\": nn_accuracy,\n",
    "            \"Inference Time GPU (µs)\": nn_avg_inference_time_gpu,\n",
    "            \"Inference Time CPU (µs)\": nn_avg_inference_time_cpu,\n",
    "            \"Memory Usage GPU (MB)\": nn_memory_usage_gpu,\n",
    "            \"Memory Usage CPU (MB)\": nn_memory_usage_cpu,\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": \"A*\",\n",
    "            \"Maze size (px)\": f\"{data_size[0]}x{data_size[1]}\",\n",
    "            \"Path Accuracy (%)\": 100,\n",
    "            \"Inference Time GPU (µs)\": \"---\",\n",
    "            \"Inference Time CPU (µs)\": alg_avg_inference_time,\n",
    "            \"Memory Usage GPU (MB)\": \"---\",\n",
    "            \"Memory Usage CPU (MB)\": alg_memory_usage,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Ergebnisse als DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train/Test Split\n",
    "path_to_data = \"./data/\"\n",
    "\n",
    "data_size = (10, 10)\n",
    "\n",
    "X_file = np.load(f\"{path_to_data}100000x{data_size[0]}x{data_size[1]}_unsolved.npy\")\n",
    "Y_file = np.load(f\"{path_to_data}100000x{data_size[0]}x{data_size[1]}_solved.npy\")\n",
    "\n",
    "X_file = (X_file - X_file.min() ) / ( X_file.max() - X_file.min())\n",
    "\n",
    "x_tensor = torch.tensor(X_file, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(Y_file, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batchsize = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10x10\n",
    "model, _, _ = createLevel6Net_10x10()\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"./archive/Level6Net_10x10_99.9900/net.pt\",\n",
    "        weights_only=True,\n",
    "    )\n",
    ")\n",
    "# 20x20\n",
    "# model, _, _ = createLevel7Net_20x20()\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         \"./archive/Level7Net_20x20_99.1000/net.pt\",\n",
    "#         weights_only=True,\n",
    "#     )\n",
    "# )\n",
    "# 30x30\n",
    "# model, _, _ = createLevel8Net_30x30()\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         \"./archive/Level8Net_30x30_89.8200/net.pt\",\n",
    "#         weights_only=True,\n",
    "#     )\n",
    "# )\n",
    "# 40x40\n",
    "# model, _, _ = createLevel8Net_40x40()\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         \"./archive/Level8Net_40x40_83.9350/net.pt\",\n",
    "#         weights_only=True,\n",
    "#     )\n",
    "# )\n",
    "# 50x50\n",
    "# model, _, _ = createLevel8Net_50x50()\n",
    "# model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         \"./archive/Level8Net_50x50_75.3600/net.pt\",\n",
    "#         weights_only=True,\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0\n",
      "round 1\n",
      "round 2\n",
      "round 3\n",
      "round 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.84432768583298"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time, _ = measure_inference_time_nn(model, test_loader)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = compare_nn_alg(test_loader, model, a_star)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
